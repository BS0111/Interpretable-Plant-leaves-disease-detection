{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LRP","metadata":{"_uuid":"48784d7a-bb36-411b-8657-a0d0be401904","_cell_guid":"68ee5792-b962-46c2-980a-53e60e2e2db0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# ---------------------------------------------\n# 1. Import required libraries\n# ---------------------------------------------\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.optim as optim\nimport copy\nimport os\nimport shutil\nfrom sklearn.model_selection import train_test_split\n\n# Check CUDA availability and set device\nprint(torch.cuda.is_available())\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# ---------------------------------------------\n# 2. Load and process the dataset\n# ---------------------------------------------\n\n# Set file paths\nCSV_FILE = \"/kaggle/input/plant-pathology-2020-fgvc7/train.csv\"\nIMAGE_DIR = \"/kaggle/input/plant-pathology-2020-fgvc7/images\"\nOUTPUT_DIR = \"/kaggle/working/processed_data\"\n\n# Load CSV into dataframe\ndf = pd.read_csv(CSV_FILE)\n\n# Create a new label column by selecting the disease type with the highest probability\ndf['label'] = df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(axis=1)\n\n# Split into training and testing datasets (80-20 split with stratification)\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n\n# Helper function to create label-wise subdirectories\ndef create_directories(root_dir, classes):\n    for class_name in classes:\n        os.makedirs(os.path.join(root_dir, class_name), exist_ok=True)\n\n# Create output directories\ntrain_output_dir = os.path.join(OUTPUT_DIR, \"train\")\ntest_output_dir = os.path.join(OUTPUT_DIR, \"test\")\ncreate_directories(train_output_dir, df['label'].unique())\ncreate_directories(test_output_dir, df['label'].unique())\n\n# Copy image files to their respective class folders\ndef organize_images(df, output_dir):\n    for _, row in df.iterrows():\n        src_path = os.path.join(IMAGE_DIR, f\"{row['image_id']}.jpg\")\n        dest_path = os.path.join(output_dir, row['label'], f\"{row['image_id']}.jpg\")\n        shutil.copy(src_path, dest_path)\n\n# Organize images into train and test folders\norganize_images(train_df, train_output_dir)\norganize_images(test_df, test_output_dir)\nprint(\"Dataset organized into train and test folders.\")\n\n# ---------------------------------------------\n# 3. Load image datasets using ImageFolder\n# ---------------------------------------------\nTRAIN_ROOT = train_output_dir\nTEST_ROOT = test_output_dir\n\ntrain_dataset = torchvision.datasets.ImageFolder(root=TRAIN_ROOT)\ntest_dataset = torchvision.datasets.ImageFolder(root=TEST_ROOT)\n\n# ---------------------------------------------\n# 4. Define the CBAM module\n# ---------------------------------------------\n\n# Basic convolution block used by spatial gate\nclass BasicConv(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation, groups, bias)\n        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=0.01) if bn else None\n        self.relu = nn.ReLU(inplace=False) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn: x = self.bn(x)\n        if self.relu: x = self.relu(x)\n        return x\n\n# Flatten module for the channel attention MLP\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n# Channel attention gate\nclass ChannelGate(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n        super(ChannelGate, self).__init__()\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(inplace=False),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n        )\n        self.pool_types = pool_types\n\n    def forward(self, x):\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                pool = F.avg_pool2d(x, (x.size(2), x.size(3)))\n            elif pool_type == 'max':\n                pool = F.max_pool2d(x, (x.size(2), x.size(3)))\n            elif pool_type == 'lp':\n                pool = F.lp_pool2d(x, 2, (x.size(2), x.size(3)))\n            elif pool_type == 'lse':\n                pool = logsumexp_2d(x)\n\n            att = self.mlp(pool)\n            channel_att_sum = att if channel_att_sum is None else channel_att_sum + att\n\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        return x * scale\n\ndef logsumexp_2d(tensor):\n    tensor_flat = tensor.view(tensor.size(0), tensor.size(1), -1)\n    s, _ = torch.max(tensor_flat, dim=2, keepdim=True)\n    return s + (tensor_flat - s).exp().sum(dim=2, keepdim=True).log()\n\n# Spatial attention gate\nclass ChannelPool(nn.Module):\n    def forward(self, x):\n        return torch.cat((torch.max(x, dim=1, keepdim=True)[0], torch.mean(x, dim=1, keepdim=True)), dim=1)\n\nclass SpatialGate(nn.Module):\n    def __init__(self):\n        super(SpatialGate, self).__init__()\n        self.compress = ChannelPool()\n        self.spatial = BasicConv(2, 1, kernel_size=7, padding=3, relu=False)\n\n    def forward(self, x):\n        x_compress = self.compress(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)\n        return x * scale\n\n# Combined CBAM module\nclass CBAM(nn.Module):\n    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False):\n        super(CBAM, self).__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n\n    def forward(self, x):\n        x_out = self.ChannelGate(x)\n        if not self.no_spatial:\n            x_out = self.SpatialGate(x_out)\n        return x_out\n\n# ---------------------------------------------\n# 5. Define VGG16 with CBAM architecture\n# ---------------------------------------------\nclass VGG16_CBAM(nn.Module):\n    def __init__(self, num_classes=4):\n        super(VGG16_CBAM, self).__init__()\n        original_vgg = models.vgg16(pretrained=True)\n\n        # Split VGG16 into 5 stages\n        self.stage1 = nn.Sequential(*original_vgg.features[:5])\n        self.stage2 = nn.Sequential(*original_vgg.features[5:10])\n        self.stage3 = nn.Sequential(*original_vgg.features[10:17])\n        self.stage4 = nn.Sequential(*original_vgg.features[17:24])\n        self.stage5 = nn.Sequential(*original_vgg.features[24:31])\n\n        # Attach CBAM after each stage\n        self.cbam1 = CBAM(64)\n        self.cbam2 = CBAM(128)\n        self.cbam3 = CBAM(256)\n        self.cbam4 = CBAM(512)\n        self.cbam5 = CBAM(512)\n\n        # Classification head\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(inplace=False),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=False),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x, explain=False):\n        x = self.stage1(x)\n        if not explain: x = self.cbam1(x)\n\n        x = self.stage2(x)\n        if not explain: x = self.cbam2(x)\n\n        x = self.stage3(x)\n        if not explain: x = self.cbam3(x)\n\n        x = self.stage4(x)\n        if not explain: x = self.cbam4(x)\n\n        x = self.stage5(x)\n        if not explain: x = self.cbam5(x)\n\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n# ---------------------------------------------\n# 6. Model training setup\n# ---------------------------------------------\nmodel = VGG16_CBAM(num_classes=4).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\nnum_epochs = 20\n\n# ---------------------------------------------\n# 7. Prepare DataLoaders with transforms\n# ---------------------------------------------\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Load training and testing datasets\ntrain_dataset = datasets.ImageFolder(TRAIN_ROOT, transform=transform)\ntest_dataset = datasets.ImageFolder(TEST_ROOT, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\nclass_names = train_dataset.classes\n\n# ---------------------------------------------\n# 8. Training loop\n# ---------------------------------------------\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss, correct, total = 0, 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, preds = outputs.max(1)\n        correct += preds.eq(labels).sum().item()\n        total += labels.size(0)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {100 * correct / total:.2f}%\")\n\n# ---------------------------------------------\n# 9. Save the trained model\n# ---------------------------------------------\ntorch.save(model.state_dict(), \"vgg16_cbam_grape_leaf_Apple.pth\")","metadata":{"_uuid":"a1479586-5e62-4ed2-b0e7-94fd11303219","_cell_guid":"9cb7c3b4-6b00-4f74-b191-131461ae1549","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---------------------------------------------\n# 10. Reload the trained model for explanation\n# ---------------------------------------------\n# Instantiate the same model architecture\nmodel = VGG16_CBAM(num_classes=4)\n# Load the saved weights from disk\nmodel.load_state_dict(torch.load(\"/kaggle/working/vgg16_cbam_grape_leaf_Apple.pth\"))\n# Ensure the model is on the correct device (GPU/CPU)\nmodel.to(device)\n\n# ---------------------------------------------\n# 11. Install and import Zennit for LRP\n# ---------------------------------------------\n# Install the Zennit library (for Layer-wise Relevance Propagation)\n!pip install zennit\n\n# Import various Zennit components for attribution\nfrom zennit.attribution import Gradient\nfrom zennit.composites import LayerMapComposite\nimport zennit.rules as z_rules\nfrom zennit.image import imgify\nfrom zennit.torchvision import VGGCanonizer\nfrom zennit.rules import ZPlus, Epsilon\nfrom zennit.composites import EpsilonPlusFlat, EpsilonPlus\n\n# ---------------------------------------------\n# 12. Prepare model and image for attribution\n# ---------------------------------------------\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\n\n# Re‐instantiate and load the model in eval mode on CPU (for explanation)\nmodel = VGG16_CBAM(num_classes=4)\nmodel.load_state_dict(torch.load(\n    '/kaggle/working/vgg16_cbam_grape_leaf_Apple.pth',\n    map_location='cpu'         # load onto CPU\n))\nmodel.eval()                   # set to evaluation mode\n\n# Load a sample image from the test set\nimage_path = '/kaggle/working/processed_data/test/rust/Train_1023.jpg'\nimage = Image.open(image_path).convert('RGB')\n\n# Define basic transform (resize + to‐tensor) for Zennit (no normalization)\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n# Add batch dimension\ninput_tensor = transform(image).unsqueeze(0)\n\n# ---------------------------------------------\n# 13. Wrapper to force CBAM bypass during explain\n# ---------------------------------------------\nfrom zennit.image import imsave, CMAPS\n\nclass ExplainWrapper(torch.nn.Module):\n    \"\"\"Wrap the model so that every forward pass uses explain=True.\"\"\"\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, x):\n        # Pass explain=True to bypass CBAM modules\n        return self.model(x, explain=True)\n\n# ---------------------------------------------\n# 14. Set up Zennit composites and run LRP\n# ---------------------------------------------\nfrom zennit.canonizers import SequentialMergeBatchNorm\nfrom zennit.composites import (\n    EpsilonGammaBox,\n    EpsilonPlus,\n    EpsilonPlusFlat,\n    EpsilonAlpha2Beta1Flat,\n    ExcitationBackprop\n)\nfrom zennit.attribution import Gradient as ZennitGradient\n\n# Wrap model for explanation\nmodel = ExplainWrapper(model)\n\n# Define canonizers required by Zennit composites\ncanonizers = [SequentialMergeBatchNorm()]\n\n# Create a dictionary of different LRP rules to apply\nzennit_composites = {\n    'epsilon_gamma_box': EpsilonGammaBox(low=-3., high=3., canonizers=canonizers),\n    'epsilon_plus': EpsilonPlus(canonizers=canonizers),\n    'epsilon_plus_flat': EpsilonPlusFlat(canonizers=canonizers),\n    'epsilon_alpha2_beta1_flat': EpsilonAlpha2Beta1Flat(canonizers=canonizers),\n    'excitation_backprop': ExcitationBackprop(canonizers=canonizers),\n}\n\n# Predict the class for the input image\nwith torch.no_grad():\n    out = model(input_tensor)               # forward pass with explain=True\n    pred_class = out.argmax().item()        # get predicted class index\n\n# Create one‐hot relevance at the output layer (shape [1, num_classes])\nrelevance_at_output = torch.eye(4)[[pred_class]]\n\n# Helper function to visualize and save the heatmap\ndef visualize_absolute(relevance, method_name):\n    # Convert tensor to numpy and sum across channels\n    rel_np = relevance.squeeze().detach().cpu().numpy()\n    rel_map = np.sum(np.abs(rel_np), axis=0)  # absolute sum over channels\n\n    plt.figure(figsize=(4, 4))\n    plt.title(method_name)\n    plt.axis('off')\n    # Display the relevance map as a heatmap\n    plt.imshow(rel_map, cmap='hot')\n    plt.colorbar()\n    plt.tight_layout()\n    # Save the figure to disk\n    plt.savefig(f'lrp_results_new_apple_/{method_name}.png')\n    plt.show()\n\n# Apply each Zennit composite and visualize\nfor name, composite in zennit_composites.items():\n    # Use ZennitGradient context manager to get relevance\n    with ZennitGradient(model, composite) as attributor:\n        _, relevance = attributor(input_tensor, relevance_at_output)\n        visualize_absolute(relevance, f\"Zennit_{name}\")\n\n# ---------------------------------------------\n# 15. Apply Captum attribution methods\n# ---------------------------------------------\nfrom captum.attr import (\n    GuidedBackprop,\n    Deconvolution,\n    IntegratedGradients,\n    GradientShap,\n    Occlusion,\n    Saliency,\n    NoiseTunnel\n)\n\n# Re‐instantiate model wrapper for Captum\nmodel = VGG16_CBAM(num_classes=4)\nmodel.load_state_dict(torch.load(model_path, map_location='cpu'))\nmodel.eval()\nmodel = ExplainWrapper(model)\n\n# Load and preprocess a second test image\nimage_path = \"/kaggle/working/processed_data/test/rust/Train_1204.jpg\"\nimage = Image.open(image_path).convert('RGB')\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\ninput_tensor = transform(image).unsqueeze(0)\n\n# Predict the class again to set Captum target\nwith torch.no_grad():\n    output = model(input_tensor)\n    pred_class = output.argmax().item()\n\n# Dictionary of Captum attribution methods to run\ncaptum_methods = {\n    \"Gradient\": Saliency(model),\n    \"IntegratedGradients\": IntegratedGradients(model),\n    \"SmoothGrad\": NoiseTunnel(Saliency(model)),  # wrapped in NoiseTunnel\n    \"GuidedBackprop\": GuidedBackprop(model),\n    \"Deconvolution\": Deconvolution(model),\n    \"Occlusion\": Occlusion(model),\n}\n\n# Compute and visualize attributions for each Captum method\nfor name, method in captum_methods.items():\n    if name == \"SmoothGrad\":\n        # SmoothGrad requires specifying the noise tunnel type\n        relevance = method.attribute(input_tensor, nt_type='smoothgrad', target=pred_class)\n    elif name == \"Occlusion\":\n        # Occlusion requires sliding window parameters\n        relevance = method.attribute(\n            input_tensor,\n            strides=(1, 3, 3),\n            sliding_window_shapes=(1, 15, 15),\n            target=pred_class\n        )\n    else:\n        # Standard attribute call for other methods\n        relevance = method.attribute(input_tensor, target=pred_class)\n\n    visualize_absolute(relevance, f\"Captum_{name}\")","metadata":{"_uuid":"4be4722b-28f0-4c6c-b53f-a5b1a2b66a1a","_cell_guid":"2926f8de-c47d-45f7-b863-b1110c5f99c1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CBAM Attention Maps","metadata":{"_uuid":"b6b09789-0004-4802-b6c2-1897c7133257","_cell_guid":"5787f329-9fb8-43e9-b850-c5758d90e7a0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport os\nimport shutil\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\n\n# ---------------------------------------------\n# 1. Define basic building blocks for CBAM\n# ---------------------------------------------\n\nclass BasicConv(nn.Module):\n    \"\"\"A Conv2d + (optional) BatchNorm + (optional) ReLU block.\"\"\"\n    def __init__(self, in_planes, out_planes, kernel_size,\n                 stride=1, padding=0, dilation=1, groups=1,\n                 relu=True, bn=True, bias=False):\n        super().__init__()\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size,\n                              stride=stride, padding=padding,\n                              dilation=dilation, groups=groups, bias=bias)\n        # BatchNorm if requested\n        self.bn = nn.BatchNorm2d(out_planes,\n                                 eps=1e-5, momentum=0.01,\n                                 affine=True) if bn else None\n        # ReLU activation if requested\n        self.relu = nn.ReLU(inplace=False) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass Flatten(nn.Module):\n    \"\"\"Flatten feature map to (batch_size, channels*height*width).\"\"\"\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\ndef logsumexp_2d(tensor):\n    \"\"\"Compute log-sum-exp over spatial dimensions for LSE pooling.\"\"\"\n    flattened = tensor.view(tensor.size(0), tensor.size(1), -1)\n    s, _ = torch.max(flattened, dim=2, keepdim=True)\n    return s + (flattened - s).exp().sum(dim=2, keepdim=True).log()\n\nclass ChannelGate(nn.Module):\n    \"\"\"Channel attention: uses multiple pooling types + small MLP.\"\"\"\n    def __init__(self, gate_channels, reduction_ratio=16,\n                 pool_types=['avg', 'max']):\n        super().__init__()\n        # MLP to compute channel weights\n        self.mlp = nn.Sequential(\n            Flatten(),\n            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n            nn.ReLU(inplace=False),\n            nn.Linear(gate_channels // reduction_ratio, gate_channels),\n        )\n        self.pool_types = pool_types\n\n    def forward(self, x):\n        channel_att_sum = None\n        # Apply each pooling type and accumulate MLP outputs\n        for p in self.pool_types:\n            if p == 'avg':\n                pool = F.avg_pool2d(x, x.shape[2:])\n            elif p == 'max':\n                pool = F.max_pool2d(x, x.shape[2:])\n            elif p == 'lp':\n                pool = F.lp_pool2d(x, 2, x.shape[2:])\n            elif p == 'lse':\n                pool = logsumexp_2d(x)\n            att_raw = self.mlp(pool)\n            channel_att_sum = att_raw if channel_att_sum is None else channel_att_sum + att_raw\n        # Sigmoid to get weights, reshape & scale\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        return x * scale\n\nclass ChannelPool(nn.Module):\n    \"\"\"Concatenate channel-wise max and mean pools along channel dim.\"\"\"\n    def forward(self, x):\n        max_pool = torch.max(x, dim=1, keepdim=True)[0]\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        return torch.cat([max_pool, avg_pool], dim=1)\n\nclass SpatialGate(nn.Module):\n    \"\"\"Spatial attention: compress along channel, then conv to produce 1-channel map.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.compress = ChannelPool()\n        # 7×7 conv to produce spatial attention map\n        self.spatial = BasicConv(2, 1, kernel_size=7, padding=3, relu=False)\n\n    def forward(self, x):\n        x_comp = self.compress(x)\n        x_out = self.spatial(x_comp)\n        scale = torch.sigmoid(x_out)  # broadcast over channels\n        return x * scale\n\nclass CBAM(nn.Module):\n    \"\"\"Convolutional Block Attention Module (CBAM): channel + (optional) spatial.\"\"\"\n    def __init__(self, gate_channels, reduction_ratio=16,\n                 pool_types=['avg', 'max'], no_spatial=False):\n        super().__init__()\n        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.SpatialGate = SpatialGate()\n\n    def forward(self, x):\n        x = self.ChannelGate(x)\n        if not self.no_spatial:\n            x = self.SpatialGate(x)\n        return x\n\n# ---------------------------------------------\n# 2. Define VGG16 + CBAM Model\n# ---------------------------------------------\nclass VGG16_CBAM(nn.Module):\n    \"\"\"VGG16 architecture with CBAM modules after each block.\"\"\"\n    def __init__(self, num_classes=4):\n        super().__init__()\n        original = models.vgg16(pretrained=True)\n        # Split VGG features into 5 stages\n        self.stage1 = nn.Sequential(*original.features[:5])\n        self.stage2 = nn.Sequential(*original.features[5:10])\n        self.stage3 = nn.Sequential(*original.features[10:17])\n        self.stage4 = nn.Sequential(*original.features[17:24])\n        self.stage5 = nn.Sequential(*original.features[24:31])\n        # CBAM modules\n        self.cbam1 = CBAM(64)\n        self.cbam2 = CBAM(128)\n        self.cbam3 = CBAM(256)\n        self.cbam4 = CBAM(512)\n        self.cbam5 = CBAM(512)\n        # Classifier head (same as VGG but adjusted for num_classes)\n        self.classifier = nn.Sequential(\n            nn.Linear(512*7*7, 4096), nn.ReLU(inplace=False), nn.Dropout(),\n            nn.Linear(4096, 4096),     nn.ReLU(inplace=False), nn.Dropout(),\n            nn.Linear(4096, num_classes)\n        )\n\n    def forward(self, x, explain=False):\n        # Apply each stage + CBAM (unless explain=True)\n        x = self.stage1(x)\n        if not explain: x = self.cbam1(x)\n        x = self.stage2(x)\n        if not explain: x = self.cbam2(x)\n        x = self.stage3(x)\n        if not explain: x = self.cbam3(x)\n        x = self.stage4(x)\n        if not explain: x = self.cbam4(x)\n        x = self.stage5(x)\n        if not explain: x = self.cbam5(x)\n        x = torch.flatten(x, 1)\n        return self.classifier(x)\n\n# ---------------------------------------------\n# 3. Prepare data directories & loaders\n# ---------------------------------------------\n# Paths\nCSV_FILE  = \"/kaggle/input/plant-pathology-2020-fgvc7/train.csv\"\nIMAGE_DIR = \"/kaggle/input/plant-pathology-2020-fgvc7/images\"\nOUTPUT_DIR= \"/kaggle/working/processed_data\"\n\n# Load labels\ndf = pd.read_csv(CSV_FILE)\ndf['label'] = df[['healthy','multiple_diseases','rust','scab']].idxmax(axis=1)\n\n# Train/test split\ntrain_df, test_df = train_test_split(df, test_size=0.2,\n                                     stratify=df['label'], random_state=42)\n\n# Create folder structure\ndef create_directories(root, classes):\n    for c in classes:\n        os.makedirs(os.path.join(root, c), exist_ok=True)\n\ntrain_dir = os.path.join(OUTPUT_DIR, \"train\")\ntest_dir  = os.path.join(OUTPUT_DIR, \"test\")\ncreate_directories(train_dir, df['label'].unique())\ncreate_directories(test_dir,  df['label'].unique())\n\n# Copy images into per-class subfolders\ndef organize_images(df_, root):\n    for _, row in df_.iterrows():\n        src = os.path.join(IMAGE_DIR, f\"{row['image_id']}.jpg\")\n        dst = os.path.join(root, row['label'], f\"{row['image_id']}.jpg\")\n        shutil.copy(src, dst)\n\norganize_images(train_df, train_dir)\norganize_images(test_df,  test_dir)\n\n# Define transforms & loaders\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\ntrain_ds = datasets.ImageFolder(train_dir, transform=transform)\ntest_ds  = datasets.ImageFolder(test_dir,  transform=transform)\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\nclass_names = train_ds.classes\n\n# ---------------------------------------------\n# 4. Load model & extract CBAM feature maps\n# ---------------------------------------------\n# Instantiate and load pretrained weights\nmodel = VGG16_CBAM(num_classes=4).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/cbam-vgg16-apple-code/vgg16_cbam_grape_leaf_Apple.pth\"))\nmodel.eval()\n\ndef extract_cbam_features(model, img_tensor):\n    \"\"\"Return CBAM outputs from all 5 stages for a single image tensor.\"\"\"\n    x = img_tensor.unsqueeze(0).to(device)\n    with torch.no_grad():\n        x = model.stage1(x); cb1 = model.cbam1(x)\n        x = model.stage2(cb1); cb2 = model.cbam2(x)\n        x = model.stage3(cb2); cb3 = model.cbam3(x)\n        x = model.stage4(cb3); cb4 = model.cbam4(x)\n        x = model.stage5(cb4); cb5 = model.cbam5(x)\n    return [cb1, cb2, cb3, cb4, cb5]\n\n# ---------------------------------------------\n# 5. Visualization helpers\n# ---------------------------------------------\ndef show_attention_map(feature_map, title=\"CBAM Output\"):\n    \"\"\"Display channel-averaged CBAM map with a colorbar.\"\"\"\n    fmap = feature_map.squeeze(0).mean(dim=0).cpu().numpy()\n    plt.imshow(fmap, cmap='jet')\n    plt.title(title)\n    plt.axis('off')\n    plt.colorbar()\n    plt.show()\n\ndef denormalize_image(tensor):\n    \"\"\"Revert normalization to display original image.\"\"\"\n    mean = torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n    std  = torch.tensor([0.229,0.224,0.225]).view(3,1,1)\n    return (tensor.cpu()*std + mean).clamp(0,1)\n\ndef overlay_cbam_on_image(img_tensor, att_map, alpha=0.5):\n    \"\"\"\n    Overlay a single CBAM map on the original image.\n    Returns (original_image, overlay_image).\n    \"\"\"\n    img = denormalize_image(img_tensor).permute(1,2,0).numpy()\n    # Resize attention map to image size\n    att = att_map.cpu().squeeze(0).mean(dim=0).numpy()\n    att = cv2.resize(att, (img.shape[1], img.shape[0]))\n    # Normalize & colorize\n    att = (att - att.min())/(att.max()-att.min())\n    heat = cv2.applyColorMap((255*att).astype(np.uint8), cv2.COLORMAP_JET)\n    heat = cv2.cvtColor(heat, cv2.COLOR_BGR2RGB)/255.0\n    # Blend\n    overlay = (1-alpha)*img + alpha*heat\n    return img, overlay\n\n# ---------------------------------------------\n# 6. Example: show CBAM maps & overlays\n# ---------------------------------------------\n# Pick an example image\nimg, lbl = test_ds[0]\nmaps = extract_cbam_features(model, img)\n\n# Display maps from each stage\nfor i, fmap in enumerate(maps, start=1):\n    show_attention_map(fmap, title=f\"CBAM Stage {i}\")\n\n# Overlay for a chosen stage\norig, ovl = overlay_cbam_on_image(img, maps[3], alpha=0.6)\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1); plt.imshow(orig); plt.title(\"Original\"); plt.axis('off')\nplt.subplot(1,2,2); plt.imshow(ovl);  plt.title(\"Overlay Stage 4\"); plt.axis('off')\nplt.show()\n\n# ---------------------------------------------\n# 7. Extended visualization with prediction\n# ---------------------------------------------\ndef visualize_cbam_with_prediction(model, dataset, index=0,\n                                   save_dir=\"./cbam_outputs\",\n                                   filename_prefix=\"cbam_pred\"):\n    \"\"\"Plot original + CBAM overlays for all stages, include model prediction.\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    model.eval()\n    img, lbl = dataset[index]\n    x = img.unsqueeze(0).to(device)\n    # Predict\n    with torch.no_grad():\n        pred = class_names[model(x).argmax(1).item()]\n    true = class_names[lbl]\n    # Extract maps\n    maps = extract_cbam_features(model, img)\n    # Plot grid: original + 5 overlays\n    fig, axs = plt.subplots(1, 6, figsize=(20,4))\n    orig_img = denormalize_image(x.squeeze(0)).permute(1,2,0).numpy()\n    axs[0].imshow(orig_img); axs[0].set_title(\"Original\"); axs[0].axis('off')\n    for i, fmap in enumerate(maps):\n        _, ov = overlay_cbam_on_image(img, fmap)\n        axs[i+1].imshow(ov); axs[i+1].set_title(f\"Stage {i+1}\"); axs[i+1].axis('off')\n    # Annotate with true vs. predicted\n    color = \"green\" if pred == true else \"red\"\n    plt.suptitle(f\"True: {true} | Predicted: {pred}\", color=color, fontsize=14)\n    plt.tight_layout(rect=[0,0.03,1,0.95])\n    # Save figure\n    path = os.path.join(save_dir, f\"{filename_prefix}_{index}.png\")\n    plt.savefig(path)\n    print(f\"✅ Saved visualization to {path}\")\n    plt.show()\n\n# Run extended visualization example\nvisualize_cbam_with_prediction(model, test_ds, index=5)","metadata":{"_uuid":"d3214f2d-46c0-49e9-a010-8057b3b9e8eb","_cell_guid":"95c63bb2-0259-430c-83c9-5ae2d5e76e95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# UMAP","metadata":{}},{"cell_type":"code","source":"\n!pip install umap-learn\n\n\nimport umap\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfeatures = []\nlabels = []\n\nmodel.eval()\nfor images, targets in tqdm(test_loader):\n    for img, label in zip(images, targets):\n        cbam_feats = extract_cbam_features(model, img.to(device))\n    \n        pooled = [torch.mean(f, dim=(2, 3)).squeeze().cpu().numpy() for f in cbam_feats]\n        flattened = np.concatenate(pooled)\n        features.append(flattened)\n        labels.append(label.item())\n\nfeatures = np.array(features)\nlabels = np.array(labels)\n\n\n\numap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\numap_result = umap_model.fit_transform(features)\n\n\ncolors = ['red', 'green', 'blue', 'orange']\nclass_names = ['Black Rot', 'Esca', 'Leaf Blight', 'Healthy']\nplt.figure(figsize=(10, 8))\nfor i, color in enumerate(colors):\n    idx = labels == i\n    plt.scatter(umap_result[idx, 0], umap_result[idx, 1], c=color, s=10, label=class_names[i])\nplt.legend()\nplt.title(\"UMAP on CBAM Attention Features\")\nplt.savefig(\"umap_cbam.png\", dpi=300, bbox_inches='tight')\nplt.show()\n\n\nplt.savefig(\"umap_CBAM_features.png\", dpi=300, bbox_inches='tight')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# t-SNE","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\n\ndef extract_cbam_features(model, image_tensor, device):\n    model.eval()\n    x = image_tensor.unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        x = model.stage1(x)\n        cbam1 = model.cbam1(x)\n\n        x = model.stage2(cbam1)\n        cbam2 = model.cbam2(x)\n\n        x = model.stage3(cbam2)\n        cbam3 = model.cbam3(x)\n\n        x = model.stage4(cbam3)\n        cbam4 = model.cbam4(x)\n\n        x = model.stage5(cbam4)\n        cbam5 = model.cbam5(x)\n\n    return [cbam1, cbam2, cbam3, cbam4, cbam5]\n\n\nfeatures = []\nlabels = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, targets in tqdm(test_loader):  \n        for img, label in zip(images, targets):\n            feature_vec = extract_cbam_feature_vector(model, img, device)\n            features.append(feature_vec)\n            labels.append(label.item())\n\nfeatures = np.array(features)  \nlabels = np.array(labels)      \n\n# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ntsne = TSNE(n_components=2, perplexity=30, random_state=42)\ntsne_features = tsne.fit_transform(features)\n\n\nplt.figure(figsize=(10, 8))\nclass_names = ['Black Rot', 'Esca', 'Leaf Blight', 'Healthy']\ncolors = ['r', 'g', 'b', 'orange']\n\nfor i, class_name in enumerate(class_names):\n    idxs = labels == i\n    plt.scatter(tsne_features[idxs, 0], tsne_features[idxs, 1], label=class_name, alpha=0.6, c=colors[i])\n\nplt.legend()\nplt.title(\"t-SNE on CBAM Attention Features\")\n\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GradCAM, GradCAM++","metadata":{"_uuid":"0b36b483-9b7f-477f-8e44-a31e2d5d92b7","_cell_guid":"6f6cf28f-a19f-4f7d-84ce-c44fa0573866","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport math\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\n# ---------------------------------------------\n# 1. Grad-CAM generation function\n# ---------------------------------------------\ndef generate_gradcam(model, input_tensor, target_layer):\n    \"\"\"\n    Compute Grad-CAM overlay for a single input image.\n    \n    Args:\n        model         : The neural network (with CBAM) for inference.\n        input_tensor  : A single image tensor of shape [C, H, W], unnormalized.\n        target_layer  : The convolutional layer to hook for activations/gradients.\n    Returns:\n        overlay       : The original image blended with the Grad-CAM heatmap.\n    \"\"\"\n    from torchvision import transforms\n\n    # Ensure batch dimension and send to device\n    img = input_tensor.unsqueeze(0).to(device)\n    model.eval()\n\n    # Dictionaries to store activations and gradients\n    activation = {}\n    grad = {}\n\n    # Forward hook: save target layer output\n    def save_activation(module, inp, out):\n        activation[\"value\"] = out\n\n    # Backward hook: save gradients wrt target layer\n    def save_grad(module, grad_in, grad_out):\n        grad[\"value\"] = grad_out[0]\n\n    # Register hooks\n    hook_a = target_layer.register_forward_hook(save_activation)\n    hook_g = target_layer.register_backward_hook(save_grad)\n\n    # Forward pass\n    output = model(img)\n    pred_class = output.argmax(dim=1)\n    # Compute gradient of score for predicted class\n    score = output[0, pred_class]\n    score.backward()\n\n    # Retrieve stored activation maps and gradients\n    activations = activation[\"value\"]   # shape [1, C, H, W]\n    gradients = grad[\"value\"]           # same shape\n\n    # Global average pool gradients over spatial dims\n    weights = gradients.mean(dim=(2, 3), keepdim=True)  # shape [1, C, 1, 1]\n    # Weight activations by pooled gradients\n    weighted_acts = activations * weights\n    # Sum over channels to get single heatmap\n    heatmap = weighted_acts.sum(dim=1).squeeze(0)\n    # Apply ReLU and normalize to [0,1]\n    heatmap = F.relu(heatmap)\n    heatmap /= heatmap.max()\n\n    # Convert to numpy and resize to original image size\n    heatmap = heatmap.detach().cpu().numpy()\n    heatmap = cv2.resize(heatmap, (img.shape[3], img.shape[2]))\n    heatmap = np.uint8(255 * heatmap)\n    # Colorize heatmap using OpenCV's JET colormap\n    heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n\n    # Denormalize original image for visualization\n    original = denormalize_image(img.squeeze(0)).permute(1, 2, 0).cpu().numpy()\n    original_uint8 = (original * 255).astype(np.uint8)\n    # Blend colored heatmap with original\n    overlay = cv2.addWeighted(original_uint8, 0.6, heatmap_color, 0.4, 0)\n\n    # Remove hooks to avoid side‐effects\n    hook_a.remove()\n    hook_g.remove()\n\n    return overlay\n\n# ---------------------------------------------\n# 2. Compare CBAM vs. Grad-CAM side by side\n# ---------------------------------------------\ndef compare_cbam_gradcam(model, dataset, index=0, target_layer=None,\n                         save_dir=\"./cbam_vs_gradcam\", prefix=\"compare\"):\n    \"\"\"\n    Plot original image, Grad-CAM overlay, and CBAM overlays for all stages.\n    Saves the combined figure to disk.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load image and true label\n    img, label = dataset[index]\n    img = img.to(device)\n    true_label = class_names[label]\n\n    # Model prediction\n    model.eval()\n    with torch.no_grad():\n        out = model(img.unsqueeze(0))\n        pred_label = class_names[out.argmax(1).item()]\n\n    # Generate Grad-CAM overlay\n    gradcam_overlay = generate_gradcam(model, img, target_layer)\n\n    # Extract CBAM feature maps and overlays\n    cbam_maps = extract_cbam_features(model, img)\n    \n    # Prepare plot: Original + Grad-CAM + CBAM Stages 1–5\n    fig, axs = plt.subplots(1, 7, figsize=(22, 4))\n\n    # 1) Original image\n    orig = denormalize_image(img).permute(1, 2, 0).cpu().numpy()\n    axs[0].imshow(orig)\n    axs[0].set_title(\"Original\")\n    axs[0].axis(\"off\")\n\n    # 2) Grad-CAM overlay\n    axs[1].imshow(gradcam_overlay)\n    axs[1].set_title(\"Grad-CAM\")\n    axs[1].axis(\"off\")\n\n    # 3–7) CBAM overlays for each stage\n    for i in range(5):\n        _, overlay = overlay_cbam_on_image(img, cbam_maps[i])\n        axs[i+2].imshow(overlay)\n        axs[i+2].set_title(f\"CBAM {i+1}\")\n        axs[i+2].axis(\"off\")\n\n    # Overall title with true vs. predicted labels\n    color = \"green\" if pred_label == true_label else \"red\"\n    plt.suptitle(f\"True: {true_label} | Pred: {pred_label}\", fontsize=14, color=color)\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n    # Save and display\n    filename = f\"{prefix}_idx{index}.png\"\n    path = os.path.join(save_dir, filename)\n    plt.savefig(path)\n    plt.show()\n    print(f\"✅ Saved comparison to: {path}\")\n\n# Choose a convolutional layer from stage5\ntarget_layer = model.stage5[-1]\ncompare_cbam_gradcam(model, test_dataset, index=138,\n                     target_layer=target_layer,\n                     save_dir=\"./cbam_vs_gradcam\",\n                     prefix=\"grape_comparison\")\n\n# ---------------------------------------------\n# 3. Install & import pytorch-grad-cam\n# ---------------------------------------------\n!pip install grad-cam --quiet\n\nfrom pytorch_grad_cam import GradCAMPlusPlus, GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# ---------------------------------------------\n# 4. Generate Grad-CAM++ overlay (using library)\n# ---------------------------------------------\n# Ensure model on correct device\nmodel.to(device)\nmodel.eval()\n\n# Instantiate GradCAM++ with our model and target layer\ncam_pp = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n\n# Select the same test image\nidx = 138\nimg_tensor, lbl = test_dataset[idx]\ninput_tensor = img_tensor.unsqueeze(0).to(device)\n\n# Compute grayscale CAM for true label\ntargets = [ClassifierOutputTarget(lbl)]\ngrayscale_cam_pp = cam_pp(input_tensor=input_tensor, targets=targets)[0]\n\n# Prepare normalized RGB image for overlay\nrgb_img = img_tensor.permute(1, 2, 0).cpu().numpy()\nrgb_img = (rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min())\n\n# Overlay CAM onto the image\noverlay_pp = show_cam_on_image(rgb_img, grayscale_cam_pp, use_rgb=True)\n\n# Display result\nplt.imshow(overlay_pp)\nplt.title(f\"Grad-CAM++ | True: {class_names[lbl]}\")\nplt.axis(\"off\")\nplt.show()\n\n# ---------------------------------------------\n# 5. Compare all methods: Grad-CAM, Grad-CAM++, CBAM\n# ---------------------------------------------\ndef compare_all_visuals(model, dataset, index, class_names, save_dir=\"final_comparison_outputs\"):\n    \"\"\"\n    Create a figure with:\n    Original | Grad-CAM | Grad-CAM++ | CBAM Stages 1–5\n    \"\"\"\n    model.eval()\n    os.makedirs(save_dir, exist_ok=True)\n\n    # Load image and true label\n    img_tensor, lbl = dataset[index]\n    input_tensor = img_tensor.unsqueeze(0).to(device)\n    true_label = class_names[lbl]\n\n    # Predict to get predicted label\n    with torch.no_grad():\n        out = model(input_tensor)\n        pred_label = class_names[out.argmax(1).item()]\n\n    # Prepare RGB image for overlays\n    rgb_img = denormalize_image(img_tensor).permute(1, 2, 0).cpu().numpy()\n\n    # 1) Grad-CAM (library)\n    gradcam = GradCAM(model=model, target_layers=[target_layer])\n    gc_map = gradcam(input_tensor=input_tensor, targets=[ClassifierOutputTarget(lbl)])[0]\n    overlay_gc = show_cam_on_image(rgb_img, gc_map, use_rgb=True)\n\n    # 2) Grad-CAM++ (library)\n    gradcam_pp = GradCAMPlusPlus(model=model, target_layers=[target_layer])\n    gcpp_map = gradcam_pp(input_tensor=input_tensor, targets=[ClassifierOutputTarget(lbl)])[0]\n    overlay_gcpp = show_cam_on_image(rgb_img, gcpp_map, use_rgb=True)\n\n    # 3) CBAM overlays\n    cbam_maps = extract_cbam_features(model, img_tensor.to(device))\n    cbam_overlays = [overlay_cbam_on_image(img_tensor.to(device), m)[1] for m in cbam_maps]\n\n    # Create subplots: total 8 images\n    fig, axes = plt.subplots(1, 8, figsize=(40, 5))\n    titles = [\"Original\", \"Grad-CAM\", \"Grad-CAM++\"] + [f\"CBAM {i}\" for i in range(1, 6)]\n    images = [rgb_img, overlay_gc, overlay_gcpp] + cbam_overlays\n\n    for ax, im, t in zip(axes, images, titles):\n        ax.imshow(im)\n        ax.set_title(t, fontsize=14)\n        ax.axis(\"off\")\n\n    # Super‐title with prediction status\n    color = \"green\" if pred_label == true_label else \"red\"\n    plt.suptitle(f\"True: {true_label} | Predicted: {pred_label}\", fontsize=18, color=color)\n    plt.tight_layout()\n    \n    # Save and show\n    out_path = os.path.join(save_dir, f\"visual_compare_idx{index}.png\")\n    plt.savefig(out_path)\n    plt.show()\n    print(f\"✅ Saved final comparison to: {out_path}\")\n\n# Run final comparison for image index 138\ncompare_all_visuals(model, test_dataset, index=138, class_names=class_names)","metadata":{"_uuid":"ca4eb480-e944-4613-b028-9ab8797eec28","_cell_guid":"dd924ce8-9c34-4984-9662-9a1c0de9978e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}